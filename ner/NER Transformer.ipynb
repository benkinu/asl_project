{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486e3675-11f4-464d-bab9-3e90ba663637",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/asl-ml-immersion/notebooks/text_models/bert_kernel/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 959.94 KiB (download: 959.94 KiB, generated: 3.87 MiB, total: 4.80 MiB) to /home/jupyter/tensorflow_datasets/conll2003/conll2003/1.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...:   0%|          | 0/4 [00:00<?, ? file/s]\u001b[A\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  2.38 url/s]3 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:00<00:00,  7.27 file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:00<00:00,  1.79 url/s]\n",
      "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]WARNING:absl:`TensorInfo.dtype` is deprecated. Please change your code to use NumPy with the field `TensorInfo.np_dtype` or use TensorFlow with the field `TensorInfo.tf_dtype`.\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.\n",
      "WARNING:absl:`TensorInfo.dtype` is deprecated. Please change your code to use NumPy with the field `TensorInfo.np_dtype` or use TensorFlow with the field `TensorInfo.tf_dtype`.\n",
      "WARNING:absl:You use TensorFlow DType <dtype: 'int64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int64.\n",
      "WARNING:absl:`TensorInfo.dtype` is deprecated. Please change your code to use NumPy with the field `TensorInfo.np_dtype` or use TensorFlow with the field `TensorInfo.tf_dtype`.\n",
      "WARNING:absl:`TensorInfo.dtype` is deprecated. Please change your code to use NumPy with the field `TensorInfo.np_dtype` or use TensorFlow with the field `TensorInfo.tf_dtype`.\n",
      "\n",
      "Generating train examples...:   0%|          | 0/14042 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating train examples...:   1%|          | 154/14042 [00:00<00:09, 1529.91 examples/s]\u001b[A\n",
      "Generating train examples...:   2%|▏         | 307/14042 [00:00<00:09, 1499.83 examples/s]\u001b[A\n",
      "Generating train examples...:   4%|▎         | 515/14042 [00:00<00:07, 1757.62 examples/s]\u001b[A\n",
      "Generating train examples...:   5%|▌         | 739/14042 [00:00<00:06, 1945.46 examples/s]\u001b[A\n",
      "Generating train examples...:   7%|▋         | 945/14042 [00:00<00:06, 1983.89 examples/s]\u001b[A\n",
      "Generating train examples...:   8%|▊         | 1144/14042 [00:00<00:06, 1867.91 examples/s]\u001b[A\n",
      "Generating train examples...:   9%|▉         | 1333/14042 [00:00<00:07, 1778.15 examples/s]\u001b[A\n",
      "Generating train examples...:  11%|█         | 1513/14042 [00:00<00:07, 1714.43 examples/s]\u001b[A\n",
      "Generating train examples...:  12%|█▏        | 1686/14042 [00:00<00:07, 1653.28 examples/s]\u001b[A\n",
      "Generating train examples...:  13%|█▎        | 1855/14042 [00:01<00:07, 1662.50 examples/s]\u001b[A\n",
      "Generating train examples...:  15%|█▍        | 2056/14042 [00:01<00:06, 1761.14 examples/s]\u001b[A\n",
      "Generating train examples...:  16%|█▌        | 2259/14042 [00:01<00:06, 1838.35 examples/s]\u001b[A\n",
      "Generating train examples...:  18%|█▊        | 2468/14042 [00:01<00:06, 1911.51 examples/s]\u001b[A\n",
      "Generating train examples...:  19%|█▉        | 2661/14042 [00:01<00:06, 1886.88 examples/s]\u001b[A\n",
      "Generating train examples...:  20%|██        | 2851/14042 [00:01<00:06, 1744.92 examples/s]\u001b[A\n",
      "Generating train examples...:  22%|██▏       | 3028/14042 [00:01<00:06, 1690.53 examples/s]\u001b[A\n",
      "Generating train examples...:  23%|██▎       | 3199/14042 [00:01<00:06, 1628.49 examples/s]\u001b[A\n",
      "Generating train examples...:  24%|██▍       | 3364/14042 [00:01<00:06, 1615.35 examples/s]\u001b[A\n",
      "Generating train examples...:  26%|██▌       | 3589/14042 [00:02<00:05, 1791.08 examples/s]\u001b[A\n",
      "Generating train examples...:  27%|██▋       | 3847/14042 [00:02<00:05, 2016.27 examples/s]\u001b[A\n",
      "Generating train examples...:  29%|██▉       | 4122/14042 [00:02<00:04, 2225.24 examples/s]\u001b[A\n",
      "Generating train examples...:  31%|███       | 4347/14042 [00:02<00:05, 1924.75 examples/s]\u001b[A\n",
      "Generating train examples...:  32%|███▏      | 4549/14042 [00:02<00:05, 1711.52 examples/s]\u001b[A\n",
      "Generating train examples...:  34%|███▍      | 4793/14042 [00:02<00:04, 1892.91 examples/s]\u001b[A\n",
      "Generating train examples...:  36%|███▌      | 5028/14042 [00:02<00:04, 2012.88 examples/s]\u001b[A\n",
      "Generating train examples...:  37%|███▋      | 5239/14042 [00:02<00:04, 2032.79 examples/s]\u001b[A\n",
      "Generating train examples...:  39%|███▉      | 5449/14042 [00:02<00:04, 2027.66 examples/s]\u001b[A\n",
      "Generating train examples...:  40%|████      | 5666/14042 [00:03<00:04, 2066.93 examples/s]\u001b[A\n",
      "Generating train examples...:  42%|████▏     | 5884/14042 [00:03<00:03, 2098.33 examples/s]\u001b[A\n",
      "Generating train examples...:  43%|████▎     | 6097/14042 [00:03<00:04, 1913.44 examples/s]\u001b[A\n",
      "Generating train examples...:  45%|████▍     | 6293/14042 [00:03<00:04, 1770.53 examples/s]\u001b[A\n",
      "Generating train examples...:  46%|████▋     | 6516/14042 [00:03<00:03, 1892.05 examples/s]\u001b[A\n",
      "Generating train examples...:  48%|████▊     | 6719/14042 [00:03<00:03, 1928.68 examples/s]\u001b[A\n",
      "Generating train examples...:  49%|████▉     | 6937/14042 [00:03<00:03, 1995.68 examples/s]\u001b[A\n",
      "Generating train examples...:  51%|█████     | 7140/14042 [00:03<00:03, 1898.72 examples/s]\u001b[A\n",
      "Generating train examples...:  52%|█████▏    | 7333/14042 [00:03<00:03, 1838.44 examples/s]\u001b[A\n",
      "Generating train examples...:  54%|█████▎    | 7519/14042 [00:04<00:03, 1761.97 examples/s]\u001b[A\n",
      "Generating train examples...:  55%|█████▍    | 7697/14042 [00:04<00:03, 1723.71 examples/s]\u001b[A\n",
      "Generating train examples...:  56%|█████▌    | 7871/14042 [00:04<00:03, 1719.50 examples/s]\u001b[A\n",
      "Generating train examples...:  57%|█████▋    | 8044/14042 [00:04<00:03, 1721.35 examples/s]\u001b[A\n",
      "Generating train examples...:  59%|█████▊    | 8227/14042 [00:04<00:03, 1750.59 examples/s]\u001b[A\n",
      "Generating train examples...:  60%|██████    | 8445/14042 [00:04<00:02, 1873.30 examples/s]\u001b[A\n",
      "Generating train examples...:  62%|██████▏   | 8640/14042 [00:04<00:02, 1893.08 examples/s]\u001b[A\n",
      "Generating train examples...:  63%|██████▎   | 8848/14042 [00:04<00:02, 1944.07 examples/s]\u001b[A\n",
      "Generating train examples...:  64%|██████▍   | 9043/14042 [00:04<00:02, 1742.89 examples/s]\u001b[A\n",
      "Generating train examples...:  66%|██████▌   | 9222/14042 [00:05<00:02, 1650.57 examples/s]\u001b[A\n",
      "Generating train examples...:  67%|██████▋   | 9398/14042 [00:05<00:02, 1676.87 examples/s]\u001b[A\n",
      "Generating train examples...:  68%|██████▊   | 9574/14042 [00:05<00:02, 1698.59 examples/s]\u001b[A\n",
      "Generating train examples...:  70%|██████▉   | 9761/14042 [00:05<00:02, 1744.43 examples/s]\u001b[A\n",
      "Generating train examples...:  71%|███████   | 9938/14042 [00:05<00:02, 1631.77 examples/s]\u001b[A\n",
      "Generating train examples...:  73%|███████▎  | 10181/14042 [00:05<00:02, 1852.11 examples/s]\u001b[A\n",
      "Generating train examples...:  74%|███████▍  | 10370/14042 [00:05<00:02, 1809.35 examples/s]\u001b[A\n",
      "Generating train examples...:  75%|███████▌  | 10554/14042 [00:05<00:01, 1787.96 examples/s]\u001b[A\n",
      "Generating train examples...:  76%|███████▋  | 10741/14042 [00:05<00:01, 1809.92 examples/s]\u001b[A\n",
      "Generating train examples...:  78%|███████▊  | 10928/14042 [00:05<00:01, 1825.88 examples/s]\u001b[A\n",
      "Generating train examples...:  79%|███████▉  | 11112/14042 [00:06<00:01, 1722.52 examples/s]\u001b[A\n",
      "Generating train examples...:  80%|████████  | 11286/14042 [00:06<00:01, 1606.57 examples/s]\u001b[A\n",
      "Generating train examples...:  82%|████████▏ | 11449/14042 [00:06<00:01, 1606.63 examples/s]\u001b[A\n",
      "Generating train examples...:  83%|████████▎ | 11612/14042 [00:06<00:01, 1554.68 examples/s]\u001b[A\n",
      "Generating train examples...:  84%|████████▍ | 11777/14042 [00:06<00:01, 1580.82 examples/s]\u001b[A\n",
      "Generating train examples...:  85%|████████▌ | 11937/14042 [00:06<00:01, 1569.75 examples/s]\u001b[A\n",
      "Generating train examples...:  86%|████████▌ | 12095/14042 [00:06<00:01, 1520.03 examples/s]\u001b[A\n",
      "Generating train examples...:  87%|████████▋ | 12253/14042 [00:06<00:01, 1536.60 examples/s]\u001b[A\n",
      "Generating train examples...:  89%|████████▊ | 12454/14042 [00:06<00:00, 1672.09 examples/s]\u001b[A\n",
      "Generating train examples...:  90%|█████████ | 12642/14042 [00:07<00:00, 1731.34 examples/s]\u001b[A\n",
      "Generating train examples...:  91%|█████████▏| 12816/14042 [00:07<00:00, 1721.86 examples/s]\u001b[A\n",
      "Generating train examples...:  93%|█████████▎| 12989/14042 [00:07<00:00, 1599.56 examples/s]\u001b[A\n",
      "Generating train examples...:  94%|█████████▎| 13151/14042 [00:07<00:00, 1553.50 examples/s]\u001b[A\n",
      "Generating train examples...:  95%|█████████▍| 13308/14042 [00:07<00:00, 1546.41 examples/s]\u001b[A\n",
      "Generating train examples...:  96%|█████████▌| 13472/14042 [00:07<00:00, 1572.26 examples/s]\u001b[A\n",
      "Generating train examples...:  97%|█████████▋| 13631/14042 [00:07<00:00, 1510.23 examples/s]\u001b[A\n",
      "Generating train examples...:  98%|█████████▊| 13797/14042 [00:07<00:00, 1551.53 examples/s]\u001b[A\n",
      "Generating train examples...: 100%|█████████▉| 13976/14042 [00:07<00:00, 1619.32 examples/s]\u001b[A\n",
      "                                                                                            \u001b[A\n",
      "Shuffling /home/jupyter/tensorflow_datasets/conll2003/conll2003/1.0.0.incompleteOK3E4V/conll2003-train.tfrecord*...:   0%|          | 0/14042 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating splits...:  33%|███▎      | 1/3 [00:08<00:16,  8.02s/ splits]                                                                                             \u001b[A\n",
      "Generating dev examples...:   0%|          | 0/3251 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating dev examples...:   6%|▌         | 192/3251 [00:00<00:01, 1913.96 examples/s]\u001b[A\n",
      "Generating dev examples...:  12%|█▏        | 388/3251 [00:00<00:01, 1934.46 examples/s]\u001b[A\n",
      "Generating dev examples...:  19%|█▉        | 618/3251 [00:00<00:01, 2098.55 examples/s]\u001b[A\n",
      "Generating dev examples...:  25%|██▌       | 828/3251 [00:00<00:01, 1869.53 examples/s]\u001b[A\n",
      "Generating dev examples...:  31%|███▏      | 1019/3251 [00:00<00:01, 1686.87 examples/s]\u001b[A\n",
      "Generating dev examples...:  37%|███▋      | 1192/3251 [00:00<00:01, 1531.88 examples/s]\u001b[A\n",
      "Generating dev examples...:  42%|████▏     | 1350/3251 [00:00<00:01, 1541.62 examples/s]\u001b[A\n",
      "Generating dev examples...:  46%|████▋     | 1507/3251 [00:00<00:01, 1533.21 examples/s]\u001b[A\n",
      "Generating dev examples...:  51%|█████     | 1663/3251 [00:01<00:01, 1523.39 examples/s]\u001b[A\n",
      "Generating dev examples...:  57%|█████▋    | 1859/3251 [00:01<00:00, 1648.36 examples/s]\u001b[A\n",
      "Generating dev examples...:  65%|██████▍   | 2109/3251 [00:01<00:00, 1896.36 examples/s]\u001b[A\n",
      "Generating dev examples...:  71%|███████   | 2302/3251 [00:01<00:00, 1890.78 examples/s]\u001b[A\n",
      "Generating dev examples...:  77%|███████▋  | 2493/3251 [00:01<00:00, 1881.55 examples/s]\u001b[A\n",
      "Generating dev examples...:  83%|████████▎ | 2683/3251 [00:01<00:00, 1758.42 examples/s]\u001b[A\n",
      "Generating dev examples...:  88%|████████▊ | 2862/3251 [00:01<00:00, 1650.04 examples/s]\u001b[A\n",
      "Generating dev examples...:  93%|█████████▎| 3030/3251 [00:01<00:00, 1549.46 examples/s]\u001b[A\n",
      "Generating dev examples...:  98%|█████████▊| 3188/3251 [00:01<00:00, 1500.30 examples/s]\u001b[A\n",
      "                                                                                        \u001b[A\n",
      "Shuffling /home/jupyter/tensorflow_datasets/conll2003/conll2003/1.0.0.incompleteOK3E4V/conll2003-dev.tfrecord*...:   0%|          | 0/3251 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating splits...:  67%|██████▋   | 2/3 [00:10<00:04,  4.47s/ splits]                                                                                          \u001b[A\n",
      "Generating test examples...:   0%|          | 0/3454 [00:00<?, ? examples/s]\u001b[A\n",
      "Generating test examples...:   6%|▌         | 199/3454 [00:00<00:01, 1972.35 examples/s]\u001b[A\n",
      "Generating test examples...:  11%|█▏        | 397/3454 [00:00<00:01, 1887.51 examples/s]\u001b[A\n",
      "Generating test examples...:  18%|█▊        | 635/3454 [00:00<00:01, 2105.41 examples/s]\u001b[A\n",
      "Generating test examples...:  25%|██▍       | 847/3454 [00:00<00:01, 2061.55 examples/s]\u001b[A\n",
      "Generating test examples...:  31%|███       | 1054/3454 [00:00<00:01, 1839.42 examples/s]\u001b[A\n",
      "Generating test examples...:  36%|███▌      | 1242/3454 [00:00<00:01, 1748.07 examples/s]\u001b[A\n",
      "Generating test examples...:  41%|████      | 1420/3454 [00:00<00:01, 1663.51 examples/s]\u001b[A\n",
      "Generating test examples...:  46%|████▌     | 1589/3454 [00:00<00:01, 1579.26 examples/s]\u001b[A\n",
      "Generating test examples...:  51%|█████     | 1751/3454 [00:01<00:01, 1590.13 examples/s]\u001b[A\n",
      "Generating test examples...:  55%|█████▌    | 1912/3454 [00:01<00:00, 1579.25 examples/s]\u001b[A\n",
      "Generating test examples...:  60%|█████▉    | 2072/3454 [00:01<00:00, 1583.64 examples/s]\u001b[A\n",
      "Generating test examples...:  65%|██████▍   | 2231/3454 [00:01<00:00, 1567.30 examples/s]\u001b[A\n",
      "Generating test examples...:  69%|██████▉   | 2389/3454 [00:01<00:00, 1545.72 examples/s]\u001b[A\n",
      "Generating test examples...:  75%|███████▌  | 2606/3454 [00:01<00:00, 1726.09 examples/s]\u001b[A\n",
      "Generating test examples...:  82%|████████▏ | 2828/3454 [00:01<00:00, 1870.60 examples/s]\u001b[A\n",
      "Generating test examples...:  89%|████████▉ | 3070/3454 [00:01<00:00, 2031.57 examples/s]\u001b[A\n",
      "Generating test examples...:  95%|█████████▌| 3291/3454 [00:01<00:00, 2083.26 examples/s]\u001b[A\n",
      "                                                                                         \u001b[A\n",
      "Shuffling /home/jupyter/tensorflow_datasets/conll2003/conll2003/1.0.0.incompleteOK3E4V/conll2003-test.tfrecord*...:   0%|          | 0/3454 [00:00<?, ? examples/s]\u001b[A\n",
      "2023-06-07 17:52:33.269215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-07 17:52:33.417629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-07 17:52:33.419433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-07 17:52:33.484735: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-07 17:52:33.527649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zer"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /home/jupyter/tensorflow_datasets/conll2003/conll2003/1.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "o\n",
      "2023-06-07 17:52:33.529554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-07 17:52:33.531185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-07 17:52:44.001467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-07 17:52:44.003482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-07 17:52:44.005218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-07 17:52:44.033172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13598 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds \n",
    "ds = tfds.load('conll2003/conll2003', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7675a11d-ebad-4cdf-be12-c8f6b7767860",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chunks': array([ 0, 17, 11, 21, 22, 11, 12, 12, 12,  0,  3, 11, 12, 12, 21, 22, 22,\n",
      "       11, 12, 12, 12,  0,  0, 21, 11, 12, 12, 12, 11, 21, 22, 22, 22,  0]), 'ner': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0]), 'pos': array([ 0, 15, 28, 41, 39, 15, 18, 11, 21,  6, 30, 29, 16, 24, 41, 30, 39,\n",
      "       11, 10, 11, 21,  6,  0, 38, 11, 22, 24, 21, 44, 38, 35, 37, 40,  7]), 'tokens': array([b'\"', b'If', b'they', b\"'re\", b'saying', b'at', b'least', b'20',\n",
      "       b'percent', b',', b'then', b'their', b'internal', b'forecasts',\n",
      "       b'are', b'probably', b'saying', b'25', b'or', b'30', b'percent',\n",
      "       b',', b'\"', b'said', b'one', b'Sydney', b'media', b'analyst',\n",
      "       b'who', b'declined', b'to', b'be', b'named', b'.'], dtype=object)}\n",
      "{'chunks': array([11, 11, 12, 21, 11, 21, 22, 13, 11, 12, 11, 12,  0, 21, 17, 11, 12,\n",
      "       21, 22, 22, 22,  1, 17, 11, 21, 22, 22, 11, 12, 13, 11, 12,  0]), 'ner': array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0]), 'pos': array([21, 27, 21, 38, 28, 20, 37, 15, 12, 21, 27, 21,  6, 39, 15, 29, 21,\n",
      "       20, 37, 40, 40, 16, 15, 28, 38, 30, 40, 12, 16, 15, 16, 21,  7]), 'tokens': array([b'Lauck', b\"'s\", b'lawyer', b'vowed', b'he', b'would', b'appeal',\n",
      "       b'against', b'the', b'court', b\"'s\", b'decision', b',', b'arguing',\n",
      "       b'that', b'his', b'client', b'should', b'have', b'been', b'set',\n",
      "       b'free', b'because', b'he', b'had', b'not', b'committed', b'any',\n",
      "       b'offence', b'under', b'German', b'law', b'.'], dtype=object)}\n",
      "{'chunks': array([11, 11, 12, 12, 21, 11, 12, 21,  1,  0, 11, 12, 12, 12, 11, 12, 21,\n",
      "       22, 11, 12, 12,  1,  2,  2, 11,  0, 11, 12, 12, 21, 13, 11,  0]), 'ner': array([5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'pos': array([22, 27, 16, 21, 42, 12, 21, 42, 16, 10, 22, 22, 22, 22, 27, 21, 20,\n",
      "       37, 12, 21, 27, 16, 10, 16, 24,  6, 12, 21, 21, 38, 15, 22,  7]), 'tokens': array([b'Thailand', b\"'s\", b'powerful', b'military', b'thinks', b'the',\n",
      "       b'government', b'is', b'dishonest', b'and', b'Prime', b'Minister',\n",
      "       b'Banharn', b'Silpa-archa', b\"'s\", b'resignation', b'might',\n",
      "       b'solve', b'the', b'nation', b\"'s\", b'political', b'and',\n",
      "       b'economic', b'woes', b',', b'an', b'opinion', b'poll', b'showed',\n",
      "       b'on', b'Thursday', b'.'], dtype=object)}\n",
      "{'chunks': array([11, 12, 12, 11, 21, 11, 12, 12, 13, 11, 12, 12, 12,  0, 11, 12, 13,\n",
      "       11, 12, 12, 11, 12, 12, 21, 13, 11, 13, 11, 12,  0, 21, 11, 21, 13,\n",
      "       11, 11, 12, 13, 11, 12, 12,  0]), 'ner': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 7,\n",
      "       0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0]), 'pos': array([12, 16, 21, 44, 38, 12, 16, 21, 15, 16, 21, 22, 22,  6, 12, 21, 15,\n",
      "       22, 22, 22, 27, 22, 21, 38, 15, 21, 15, 12, 22,  6, 38, 28, 38, 15,\n",
      "       21, 12, 21, 15, 12, 16, 21,  7]), 'tokens': array([b'A', b'forensic', b'scientist', b'who', b'examined', b'the',\n",
      "       b'supposed', b'skull', b'of', b'19th', b'century', b'King',\n",
      "       b'Hintsa', b',', b'a', b'chief', b'of', b'President', b'Nelson',\n",
      "       b'Mandela', b\"'s\", b'Xhosa', b'tribe', b'killed', b'in', b'battle',\n",
      "       b'by', b'the', b'British', b',', b'said', b'it', b'was', b'in',\n",
      "       b'fact', b'the', b'cranium', b'of', b'a', b'European', b'woman',\n",
      "       b'.'], dtype=object)}\n",
      "{'chunks': array([11, 12, 12, 12, 12, 12, 12, 12, 12]), 'ner': array([3, 4, 0, 0, 0, 0, 0, 0, 0]), 'pos': array([22, 22, 11, 11, 11, 11, 11, 11, 11]), 'tokens': array([b'Werder', b'Bremen', b'3', b'0', b'1', b'2', b'4', b'6', b'1'],\n",
      "      dtype=object)}\n",
      "{'chunks': array([11, 12, 12, 12, 12]), 'ner': array([3, 0, 0, 0, 0]), 'pos': array([22, 11, 11, 11, 11]), 'tokens': array([b'SEATTLE', b'66', b'63', b'.512', b'8'], dtype=object)}\n",
      "{'chunks': array([11,  0, 13, 11, 12,  0, 11, 12,  0, 11]), 'ner': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'pos': array([16,  4, 15, 11, 21,  8, 11, 24,  5, 11]), 'tokens': array([b'Total', b'(', b'for', b'one', b'wicket', b'-', b'44.2', b'overs',\n",
      "       b')', b'230'], dtype=object)}\n",
      "{'chunks': array([21, 11, 11, 21, 22, 11, 12, 12, 12, 12, 13, 11, 21, 22, 11, 12, 12,\n",
      "       12, 13, 11, 12, 12, 12,  0]), 'ner': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0]), 'pos': array([38, 22, 28, 38, 40, 12,  3, 11, 11, 21, 15, 22, 35, 37, 16, 21, 16,\n",
      "       24, 15, 29, 21, 21, 21,  7]), 'tokens': array([b'said', b'Thursday', b'it', b'had', b'signed', b'a', b'$', b'1',\n",
      "       b'billion', b'contract', b'with', b'Zenith', b'to', b'make',\n",
      "       b'digital', b'televison', b'set-top', b'boxes', b'for', b'its',\n",
      "       b'home', b'entertainment', b'service', b'.'], dtype=object)}\n",
      "{'chunks': array([ 0, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]), 'ner': array([0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0]), 'pos': array([ 8, 22, 22, 22, 22, 11, 11, 11, 21, 11, 11, 11]), 'tokens': array([b'-', b'Air', b'Cargo', b'Newsroom', b'Tel+44', b'171', b'542',\n",
      "       b'7706', b'Fax+44', b'171', b'542', b'5017'], dtype=object)}\n",
      "{'chunks': array([21, 11, 12, 12, 12, 12, 12, 12, 12]), 'ner': array([3, 0, 0, 0, 0, 0, 0, 0, 0]), 'pos': array([39, 11, 11, 11, 11, 11, 11, 11, 11]), 'tokens': array([b'Geelong', b'21', b'13', b'1', b'7', b'2288', b'1940', b'117.9',\n",
      "       b'54'], dtype=object)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 18:07:06.550759: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in ds.take(10).as_numpy_iterator():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1ed37-e8b0-4deb-960a-7bda31ae381a",
   "metadata": {},
   "source": [
    "\n",
    "## Title: Named Entity Recognition using Transformers\n",
    "* Author: [Varun Singh](https://www.linkedin.com/in/varunsingh2/)\n",
    "* Date created: Jun 23, 2021\n",
    "* Last modified: Jun 24, 2021\n",
    "* Description: NER using the Transformers and data from CoNLL 2003 shared task.\n",
    "* Accelerator: GPU\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Named Entity Recognition (NER) is the process of identifying named entities in text.\n",
    "Example of named entities are: \"Person\", \"Location\", \"Organization\", \"Dates\" etc. NER is\n",
    "essentially a token classification task where every token is classified into one or more\n",
    "predetermined categories.\n",
    "\n",
    "In this exercise, we will train a simple Transformer based model to perform NER. We will\n",
    "be using the data from CoNLL 2003 shared task. For more information about the dataset,\n",
    "please visit [the dataset website](https://www.clips.uantwerpen.be/conll2003/ner/).\n",
    "However, since obtaining this data requires an additional step of getting a free license, we will be using\n",
    "HuggingFace's datasets library which contains a processed version of this dataset.\n",
    "\n",
    "\n",
    "## Install the open source datasets library from HuggingFace\n",
    "\n",
    "We also download the script used to evaluate NER models.\n",
    "\n",
    "\n",
    "shell\n",
    "```\n",
    "pip3 install datasets\n",
    "wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bc117c79-5c27-4265-ac5f-60633f35fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "from conlleval import evaluate\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e389779b-4530-49f7-91a9-acf4c2919784",
   "metadata": {},
   "source": [
    "We will be using the transformer implementation from this fantastic\n",
    "[example](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
    "\n",
    "Let's start by defining a `TransformerBlock` layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e45a375-d453-43a1-bf33-b37cb1bb54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                keras.layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = keras.layers.Dropout(rate)\n",
    "        self.dropout2 = keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f828f3e-0fc4-4df4-886a-7e45f9b81c41",
   "metadata": {},
   "source": [
    "Next, let's define a `TokenAndPositionEmbedding` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19813949-2320-4880-ae38-4f0b24cd51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = keras.layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        maxlen = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        position_embeddings = self.pos_emb(positions)\n",
    "        token_embeddings = self.token_emb(inputs)\n",
    "        return token_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97557e32-23b4-4af2-93aa-06f50246f4d1",
   "metadata": {},
   "source": [
    "## Build the NER model class as a `keras.Model` subclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7dc58ca-be45-4b92-969c-a3820058f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.transformer_block1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.transformer_block2 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.transformer_block3 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.transformer_block4 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.transformer_block5 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "        self.transformer_block6 = TransformerBlock(embed_dim, num_heads, ff_dim)        \n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.transformer_block1(x)\n",
    "        x = self.transformer_block2(x)\n",
    "        x = self.transformer_block3(x)        \n",
    "        x = self.transformer_block4(x)\n",
    "        x = self.transformer_block5(x)\n",
    "        x = self.transformer_block6(x)        \n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.ff_final(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4344755f-d69b-473f-bfd8-636872dd2db0",
   "metadata": {},
   "source": [
    "## Load the CoNLL 2003 dataset from the datasets library and process it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08d732ba-7e25-49fe-98dc-ac080d73993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/home/jupyter/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n",
      "100%|██████████| 3/3 [00:00<00:00, 101.77it/s]\n"
     ]
    }
   ],
   "source": [
    "conll_data = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7239c2f-b14f-4b2f-8e5a-879ebcb9ee05",
   "metadata": {},
   "source": [
    "We will export this data to a tab-separated file format which will be easy to read as a\n",
    "`tf.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0465031c-ddd1-4c3e-b2d3-a934ab37254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_file(export_file_path, data):\n",
    "    with open(export_file_path, \"w\") as f:\n",
    "        for record in data:\n",
    "            ner_tags = record[\"ner_tags\"]\n",
    "            tokens = record[\"tokens\"]\n",
    "            if len(tokens) > 0:\n",
    "                f.write(\n",
    "                    str(len(tokens))\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(tokens)\n",
    "                    + \"\\t\"\n",
    "                    + \"\\t\".join(map(str, ner_tags))\n",
    "                    + \"\\n\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f1bb8e-4cbd-4102-b798-d90600577449",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_29974/666389683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mexport_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/conll_train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconll_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexport_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/conll_val.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconll_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data'"
     ]
    }
   ],
   "source": [
    "os.mkdir(\"data\")\n",
    "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
    "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2e4cf-59e8-47d0-a033-9cec072e380f",
   "metadata": {},
   "source": [
    "## Make the NER label lookup table\n",
    "\n",
    "NER labels are usually provided in IOB, IOB2 or IOBES formats. Checkout this link for\n",
    "more information:\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n",
    "\n",
    "Note that we start our label numbering from 1 since 0 will be reserved for padding. We\n",
    "have a total of 10 labels: 9 from the NER dataset and one for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2c367d-0259-42fb-9344-2be0760c513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tag_lookup_table():\n",
    "    iob_labels = [\"B\", \"I\"]\n",
    "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
    "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
    "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
    "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
    "    return dict(zip(range(0, len(all_labels) + 1), all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740f1cfa-4bc4-454d-87e5-1b43f19ee5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
     ]
    }
   ],
   "source": [
    "mapping = make_tag_lookup_table()\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfd372-2f03-437d-ac65-ef85070d34ee",
   "metadata": {},
   "source": [
    "Get a list of all tokens in the training dataset. This will be used to create the\n",
    "vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c91a7ce-1638-4ad0-bc5a-4ac8bf114180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21009\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
    "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
    "\n",
    "counter = Counter(all_tokens_array)\n",
    "print(len(counter))\n",
    "\n",
    "num_tags = len(mapping)\n",
    "vocab_size = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681427b-ce98-4227-b7ca-27ff3bc0357e",
   "metadata": {},
   "source": [
    "We only take (vocab_size - 2) most commons words from the training data since the `StringLookup` class uses 2 additional tokens - one denoting an unknown token and another one denoting a masking token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b307cb7-2634-4d2f-8dbe-c051a68cd2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9a49e-5619-4eac-8211-079820bb5faf",
   "metadata": {},
   "source": [
    "The StringLook class will convert tokens to token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d38e81-d6e8-4cdf-ae0e-832b96eae75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 03:38:55.202807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:38:55.259757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:38:55.261364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:38:55.292736: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 03:38:55.324872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:38:55.326585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:38:55.328209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:39:05.778076: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:39:05.779935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:39:05.781488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-08 03:39:05.802393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13598 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23045b23-9c5f-43d8-a764-003b25bb5460",
   "metadata": {},
   "source": [
    "Create 2 new `Dataset` objects from the training and validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cfa2c0b-b990-4230-8de9-3509f008d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
    "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1dc223-618d-4a5f-aba7-e2a1f5439135",
   "metadata": {},
   "source": [
    "Print out one line to make sure it looks good. The first record in the line is the number of tokens. \n",
    "After that we will have all the tokens followed by all the ner tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "540594d5-6316-423a-8bb5-564d403cdc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0']\n"
     ]
    }
   ],
   "source": [
    "print(list(train_data.take(1).as_numpy_iterator()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b74d59f-aef4-427d-b65c-2e55bf5f6f24",
   "metadata": {},
   "source": [
    "We will be using the following map function to transform the data in the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c80eb87-7929-4153-899c-9597fb00a47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_record_to_training_data(record):\n",
    "    record = tf.strings.split(record, sep=\"\\t\")\n",
    "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
    "    tokens = record[1 : length + 1]\n",
    "    tags = record[length + 1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d798f7d-f061-428f-a2db-7b1ac7458b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_record_to_training_data_with_bert(record_input):\n",
    "    records = tf.strings.split(record_input, sep=\"\\t\")\n",
    "    length = tf.strings.to_number(records[0], out_type=tf.int32)\n",
    "    value = records[1 : length + 1]\n",
    "    value1 = tf.split(value, num_or_size_splits = value.shape[1], axis = 1)\n",
    "    input = tf.strings.join(value1, \" \")\n",
    "    \n",
    "    tags = records[length + 1 :]\n",
    "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
    "    tags += 1\n",
    "    return input, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ccb73af-2fbe-4445-a584-46d087d94ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_and_convert_to_ids(tokens):\n",
    "    tokens = tf.strings.lower(tokens)\n",
    "    return lookup_layer(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46008f9e-96ec-4a78-be43-aba088e669aa",
   "metadata": {},
   "source": [
    "We use `padded_batch` here because each record in the dataset has a different length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30bc0611-735d-4d2a-88d1-427a152c1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb5817d2-22c1-4627-ad11-66d451f1e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    train_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "237cdd16-c48a-43d8-afdd-98fea7916d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = (\n",
    "    val_data.map(map_record_to_training_data)\n",
    "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
    "    .padded_batch(batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7a968cd-a10e-4b64-a0eb-9fd9e8142467",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "10977ddf-17cc-449b-8421-00deb61bbeb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_29974/3153947928.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mner_model_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNERModelBert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/tmp/ipykernel_29974/3730781269.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dropout_rate, tfhub_handle_encoder, tfhub_handle_preprocess, base_url)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtfhub_handle_preprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"preprocessing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtfhub_handle_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BERT_encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "ner_model_bert = NERModelBert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d114891-df64-4a25-b154-5059aa8a604f",
   "metadata": {},
   "source": [
    "We will be using a custom loss function that will ignore the loss from padded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce1244ba-b223-4065-ae03-691bd266e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_ner_loss\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
    "        )\n",
    "        loss = loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1314fc2a-4da7-42c2-9ceb-4d89a67a65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CustomNonPaddingTokenLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf9d21b-224b-4e48-994d-4f4cf9dd57fe",
   "metadata": {},
   "source": [
    "## Compile and fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e486f631-b09b-4214-b6f3-f320aab431ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "439/439 [==============================] - 15s 23ms/step - loss: 0.9263\n",
      "Epoch 2/10\n",
      "439/439 [==============================] - 10s 24ms/step - loss: 0.6021\n",
      "Epoch 3/10\n",
      "439/439 [==============================] - 11s 24ms/step - loss: 0.2744\n",
      "Epoch 4/10\n",
      "439/439 [==============================] - 10s 23ms/step - loss: 0.1691\n",
      "Epoch 5/10\n",
      "439/439 [==============================] - 10s 23ms/step - loss: 0.1275\n",
      "Epoch 6/10\n",
      "439/439 [==============================] - 10s 23ms/step - loss: 0.0994\n",
      "Epoch 7/10\n",
      "439/439 [==============================] - 10s 23ms/step - loss: 0.0832\n",
      "Epoch 8/10\n",
      "439/439 [==============================] - 11s 24ms/step - loss: 0.0697\n",
      "Epoch 9/10\n",
      "439/439 [==============================] - 10s 23ms/step - loss: 0.0613\n",
      "Epoch 10/10\n",
      "439/439 [==============================] - 10s 22ms/step - loss: 0.0549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7bec64b4d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
    "ner_model.fit(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba321716-348d-4b1c-ac41-ecaf5f8b6198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_ids(text):\n",
    "    tokens = text.split()\n",
    "    return lowercase_and_convert_to_ids(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24c13fe-5116-4a4b-a909-286ae893bc59",
   "metadata": {},
   "source": [
    "Sample inference using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "631ff447-32d1-4da0-8cbf-0ce6ff62095e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[  988 10950   204   628     6  3938   215  5773]], shape=(1, 8), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "sample_input = tokenize_and_convert_to_ids(\n",
    "    \"eu rejects german call to boycott british lamb\"\n",
    ")\n",
    "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53ab14a6-91ce-474b-877b-4e301c849686",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ner_model.predict(sample_input)\n",
    "prediction = np.argmax(output, axis=-1)[0]\n",
    "prediction = [mapping[i] for i in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ad2999c-cf9a-4845-a897-a8b04d1bc6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab48eee-8f79-4f1e-aef7-411dd5c1aef0",
   "metadata": {},
   "source": [
    "## Metrics calculation\n",
    "\n",
    "Here is a function to calculate the metrics. The function calculates F1 score for the\n",
    "overall NER dataset as well as individual scores for each NER tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "077f4107-db6d-4006-a233-9188168e0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(dataset):\n",
    "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
    "\n",
    "    for x, y in dataset:\n",
    "        output = ner_model.predict(x)\n",
    "        predictions = np.argmax(output, axis=-1)\n",
    "        predictions = np.reshape(predictions, [-1])\n",
    "\n",
    "        true_tag_ids = np.reshape(y, [-1])\n",
    "\n",
    "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
    "        true_tag_ids = true_tag_ids[mask]\n",
    "        predicted_tag_ids = predictions[mask]\n",
    "\n",
    "        all_true_tag_ids.append(true_tag_ids)\n",
    "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
    "\n",
    "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
    "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
    "\n",
    "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
    "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
    "\n",
    "    evaluate(real_tags, predicted_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c722734-2444-48e7-8551-61eaf0c7f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51362 tokens with 5942 phrases; found: 5295 phrases; correct: 3855.\n",
      "accuracy:  62.69%; (non-O)\n",
      "accuracy:  93.39%; precision:  72.80%; recall:  64.88%; FB1:  68.61\n",
      "              LOC: precision:  83.45%; recall:  79.86%; FB1:  81.61  1758\n",
      "             MISC: precision:  74.45%; recall:  65.73%; FB1:  69.82  814\n",
      "              ORG: precision:  65.34%; recall:  61.00%; FB1:  63.09  1252\n",
      "              PER: precision:  65.53%; recall:  52.33%; FB1:  58.19  1471\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c03cbd45-239e-4211-888c-b65f40718866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 103 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f786816bf80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "processed 51362 tokens with 5942 phrases; found: 5947 phrases; correct: 4056.\n",
      "accuracy:  65.99%; (non-O)\n",
      "accuracy:  93.18%; precision:  68.20%; recall:  68.26%; FB1:  68.23\n",
      "              LOC: precision:  74.17%; recall:  83.94%; FB1:  78.75  2079\n",
      "             MISC: precision:  75.56%; recall:  61.71%; FB1:  67.94  753\n",
      "              ORG: precision:  58.39%; recall:  61.74%; FB1:  60.02  1418\n",
      "              PER: precision:  65.82%; recall:  60.64%; FB1:  63.13  1697\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73afca4a-2abc-49f4-943d-b7c9060fc863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7103ca9b-3157-400b-8680-506cf4f287b2",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this exercise, we created a simple transformer based named entity recognition model.\n",
    "We trained it on the CoNLL 2003 shared task data and got an overall F1 score of around 70%.\n",
    "State of the art NER models fine-tuned on pretrained models such as BERT or ELECTRA can easily\n",
    "get much higher F1 score -between 90-95% on this dataset owing to the inherent knowledge\n",
    "of words as part of the pretraining process and the usage of subword tokenization.\n",
    "\n",
    "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/ner-with-transformers)\n",
    "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/ner_with_transformers).\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8514c3f3-77b8-4b0e-a900-f433eadd7a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(ner_model, to_file=\"a.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26ff9023-7629-4c5f-af7f-7e0e2377608e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f774159e-e93b-4828-af84-5a407edcebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "790df97b-e817-40e6-b0d9-fd7d3ad73773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'EU' b'rejects' b'German' b'call' b'to' b'boycott' b'British' b'lamb'\n",
      " b'.']\n"
     ]
    }
   ],
   "source": [
    "for x,y  in train_data.map(map_record_to_training_data).take(1).as_numpy_iterator():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8efeaa3b-3968-4231-a139-0a93bfe180a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0'\n"
     ]
    }
   ],
   "source": [
    "for x  in train_data.take(1).as_numpy_iterator():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "59a304af-4040-4474-8935-0f8fd30701fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = tf.constant([[[b'a',b'b']]])\n",
    "split = tf.split(value, num_or_size_splits = value.shape[1], axis = 1)\n",
    "string = tf.strings.join(split,\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "311cf14a-af69-4efb-ade7-64959056f283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 2), dtype=string, numpy=array([[[b'a', b'b']]], dtype=object)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00463e37-6760-44b8-aa72-d694251b0f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "bert_kernel",
   "name": "tf2-gpu.2-10.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m108"
  },
  "kernelspec": {
   "display_name": "bert_kernel",
   "language": "python",
   "name": "bert_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
